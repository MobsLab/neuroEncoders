#!env/bin/python3
# neuroEncoder bys the MOBS team.
# may 2020
# t.balenbois@gmail.com

import sys
import os.path
import subprocess
import numpy as np
import tensorflow as tf
from contextlib import ExitStack


class Project():
    def __init__(self, xmlPath, datPath='', jsonPath=None):
        if xmlPath[-3:] != "xml":
            if os.path.isfile(xmlPath[:-3]+"xml"):
                xmlPath = xmlPath[:-3]+"xml"
            else:
                raise ValueError("the path "+xmlPath+" doesn't match a .xml file")
        self.xml = xmlPath
        self.baseName = xmlPath[:-4]
        if datPath == '':
            self.dat = self.baseName + '.dat'
        else:
            self.dat = datPath
        findFolder = lambda path: path if path[-1]=='/' or len(path)==0 else findFolder(path[:-1]) 
        self.folder = findFolder(self.dat)
        self.fil = self.dat[:-4] + '.fil'
        if jsonPath == None:
            self.json = self.baseName + '.json'
            self.graph = self.folder + 'graph/decoder'
            self.graphMeta = self.folder + 'graph/decoder.meta'
        else:
            print('using file:',jsonPath)
            self.json = jsonPath
            self.thresholds, self.graph = self.getThresholdsAndGraph()
            self.graphMeta = self.graph + '.meta'

        self.tfrec = {
            "train": self.folder + 'dataset/trainingDataset.tfrec', 
            "test": self.folder + 'dataset/testingDataset.tfrec'}

        self.resultsNpz = self.folder + 'results/inferring.npz'
        self.resultsMat = self.folder + 'results/inferring.mat'

        if not os.path.isdir(self.folder + 'dataset'):
            os.makedirs(self.folder + 'dataset')
        if not os.path.isdir(self.folder + 'graph'):
            os.makedirs(self.folder + 'graph')
        if not os.path.isdir(self.folder + 'results'):
            os.makedirs(self.folder + 'results')

    def clu(self, g):
        return self.baseName + ".clu." + str(g+1)

    def res(self, g):
        return self.baseName + ".res." + str(g+1)

    def pos(self, g):
        return self.folder + "dataset/pos." + str(g+1) + ".npz"

    def getThresholdsAndGraph(self):
        import json
        with open(self.json, 'r') as f:
            info = json.loads(f.read())
        return [[abs(info[d][f]) for f in ['threshold'+str(c) for c in range(info[d]['nChannels'])]] \
                for d in ['group'+str(g) for g in range(info['nGroups'])]], \
            info['encodingPrefix']


class Params:
    def __init__(self, detector, windowSize):
        self.nGroups = detector.nGroups()
        self.dim_output = detector.dim_output()
        self.nChannels = detector.numChannelsPerGroup()
        self.length = 0

        self.nSteps = int(10000 * 0.036 / windowSize)
        self.nEpochs = 10
        self.learningTime = detector.learningTime()
        self.windowLength = windowSize # in seconds, as all things should be

        ### from units encoder params
        self.validCluWindow = 0.0005
        self.kernel = 'epanechnikov'
        self.bandwidth = 0.1
        self.masking = 20

        ### full encoder params
        self.nFeatures = 128
        self.lstmLayers = 3
        self.lstmSize = 128
        self.lstmDropout = 0.3

        self.batch_size = 52
        self.timeMajor = True

        self.learningRates = [0.0003, 0.00003, 0.00001]
        self.lossLearningRate = 0.00003
        self.lossActivation = None



def main(args):
    from importData import rawDataParser
    from fullEncoder import nnUtils
    from unitClassifier import bayesUtils

    if args.mode=="decode":
        jsonPath = os.path.expanduser(args.jsonPath)
    else:
        jsonPath = None
    if args.gpu:
        device_name = "/gpu:0"
    else:
        device_name = "/cpu:0"
    print('NEUROENCODER: DEVICE', device_name)
    xmlPath = args.path.strip('\'')
    datPath = args.dat
    print('NEUROENCODER: PATH', xmlPath)
    useOpenEphysFilter = not args.filter
    print('NEUROENCODER: FILTERING', not useOpenEphysFilter)
    windowSize = args.window
    print('NEUROENCODER: WINDOW', windowSize)
    mode = args.mode
    print('NEUROENCODER: MODE', mode)
    split = args.split
    print('NEUROENCODER: TEST SPLIT', split)
    print()

    trainLosses = []
    projectPath = Project(os.path.expanduser(xmlPath), datPath=os.path.expanduser(datPath), jsonPath=jsonPath)
    spikeDetector = rawDataParser.SpikeDetector(projectPath, useOpenEphysFilter, mode)
    params = Params(spikeDetector, windowSize)
    if mode == "decode":
        spikeDetector.setThresholds(projectPath.thresholds)

    # Create data files if not present
    if not os.path.isfile(projectPath.tfrec["test"]) or args.B:

        # setup data readers and writers meta data
        spikeGen = nnUtils.spikeGenerator(projectPath, spikeDetector, maxPos=spikeDetector.maxPos())
        spikeSequenceGen = nnUtils.getSpikeSequences(params, spikeGen())
        readers = {}
        writers = {"testSequences": tf.python_io.TFRecordWriter(projectPath.tfrec["test"])}
        if mode=="full":
            writers.update({"trainSequences": tf.python_io.TFRecordWriter(projectPath.tfrec["train"])})
        elif mode=="fromUnits":
            readers.update({"clu"+str(g): open(projectPath.clu(g), 'r') for g in range(params.nGroups)})
            readers.update({"res"+str(g): open(projectPath.res(g), 'r') for g in range(params.nGroups)})
            writers.update({"trainGroup"+str(g): tf.python_io.TFRecordWriter(projectPath.tfrec["train"]+"."+str(g)) for g in range(params.nGroups)})

        with ExitStack() as stack:
            # Open data files
            for k,v in writers.items():
                writers[k] = stack.enter_context(v)
            for k,v in readers.items():
                readers[k] = stack.enter_context(v)

            if mode=="fromUnits":
                nClusters = [int(readers["clu"+str(g)].readline()) for g in range(params.nGroups)]
                params.nClusters = nClusters
                clusterPositions = [{"clu"+str(n):[] for n in range(params.nClusters[g])} for g in range(params.nGroups)]
                clusterReaders = [bayesUtils.ClusterReader(readers["clu"+str(g)], readers["res"+str(g)], spikeDetector.samplingRate) \
                    for g in range(params.nGroups)]
                [clusterReaders[g].getNext() for g in range(params.nGroups)]

            # generate spike sequences in windows of size params.windowLength
            for example in spikeSequenceGen:
                if example["train"] == None:
                    continue
                if example["train"]:
                    if mode=="full":
                        writers["trainSequences"].write(nnUtils.serializeSpikeSequence(
                            params, 
                            *tuple(example[k] for k in ["pos", "groups", "length", "times"]+["spikes"+str(g) for g in range(params.nGroups)])))
                    else:
                        # If decoding from units, we need to find a sorted spike with corresponding timestamp
                        for spk in range(example["length"]):
                            group = example["groups"][spk]
                            while clusterReaders[group].res < example["times"][spk] - params.validCluWindow:
                                clusterReaders[group].getNext()
                            if clusterReaders[group].res > example["times"][spk] + params.validCluWindow:
                                continue
                            clusterPositions[group]["clu"+str(clusterReaders[group].clu)].append(example["pos"])
                            writers["trainGroup"+str(group)].write(nnUtils.serializeSingleSpike(
                                params,
                                clusterReaders[group].clu,
                                example["spikes"+str(group)][(np.array(example["groups"])==group)[:spk+1].sum()-1]))
                            clusterReaders[group].getNext()

                else:
                    writers["testSequences"].write(nnUtils.serializeSpikeSequence(
                        params, 
                        *tuple(example[k] for k in ["pos", "groups", "length", "times"]+["spikes"+str(g) for g in range(params.nGroups)])))

        if mode=="fromUnits":
            for g in range(params.nGroups):
                for c in range(params.nClusters[g]):
                    clusterPositions[g]["clu"+str(c)] = np.array(clusterPositions[g]["clu"+str(c)])
                np.savez(projectPath.pos(g), **clusterPositions[g])


    # Training, testing, and preparing network for online setup
    if mode=="full":
        from fullEncoder import nnTraining as Training
    elif mode=="fromUnits":
        from unitClassifier import bayesTraining as Training
    elif mode=="decode":
        from decoder import decodeTraining as Training
    trainer = Training.Trainer(projectPath, params, spikeDetector, device_name=device_name)
    trainLosses = trainer.train()
    outputs = trainer.test()



    # Saving files
    np.savez(projectPath.resultsNpz, trainLosses=trainLosses, **outputs)

    import scipy.io
    scipy.io.savemat(projectPath.resultsMat, np.load(projectPath.resultsNpz, allow_pickle=True))

    import json
    outjsonStr = {};
    outjsonStr['encodingPrefix'] = projectPath.graph
    outjsonStr['defaultFilter'] = not useOpenEphysFilter
    outjsonStr['mousePort'] = 0

    outjsonStr['nGroups'] = int(params.nGroups)
    idx=0
    for group in range(len(spikeDetector.list_channels)):
        outjsonStr['group'+str(group-idx)]={}
        outjsonStr['group'+str(group-idx)]['nChannels'] = len(spikeDetector.list_channels[group])
        for chnl in range(len(spikeDetector.list_channels[group])):
            outjsonStr['group'+str(group-idx)]['channel'+str(chnl)]=int(spikeDetector.list_channels[group][chnl])
            outjsonStr['group'+str(group-idx)]['threshold'+str(chnl)]=int(spikeDetector.getThresholds()[group][chnl])

    outjsonStr['nStimConditions'] = 1
    outjsonStr['stimCondition0'] = {}
    outjsonStr['stimCondition0']['stimPin'] = 14
    outjsonStr['stimCondition0']['lowerX'] = 0.0
    outjsonStr['stimCondition0']['higherX'] = 0.0
    outjsonStr['stimCondition0']['lowerY'] = 0.0
    outjsonStr['stimCondition0']['higherY'] = 0.0
    outjsonStr['stimCondition0']['lowerDev'] = 0.0
    outjsonStr['stimCondition0']['higherDev'] = 0.0

    outjson = json.dumps(outjsonStr, indent=4)
    with open(projectPath.json,"w") as json_file:
        json_file.write(outjson)

    subprocess.run(["./createOpenEphysTemplateFromJson.sh", projectPath.json])

    from fullEncoder import printResults
    printResults.printResults(projectPath.folder)

if __name__=="__main__":
    print()
    import argparse
    parser = argparse.ArgumentParser(description="Creating and training an agent to decode high level features from electrophysiology data")
    subparsers = parser.add_subparsers(dest='mode', title='modes', description='all existing modes of encoding', help='selects an encoding mode')
    for cmd in ['full', 'fromUnits', 'decode']:
        p = subparsers.add_parser(cmd)
        p.add_argument('path', type=str, help="path to xml file")
        if cmd == 'decode':
            p.add_argument('jsonPath', type=str, help="path to json file")
        p.add_argument('-d', '--dat', type=str, default='', help="path to .dat file. If not provided, default will be same name as xml in same folder")
        p.add_argument('-g', '--gpu', action='store_true', help="run computations on gpu. Requires specific installation.")
        p.add_argument('-f', '--filter', action='store_true', help="signify that neuroEncoder should filter itself, and not rely on previously filtered data.")
        p.add_argument('-w', '--window', type=float, help='defines window size, in seconds. Defaults to 0.036', default=0.036)
        p.add_argument('-s', '--split', type=float, help='defines how much data goes to test epoch, the rest goes to train epoch. Defaults to 0.1', default=0.1)
        p.add_argument('-wh', '--whichsplit', type=str, help='defines where in the data test split is taken from: beginning ("beg") or end ("end"). Defaults to "end"', default='end')
        p.add_argument('-t', '--target', type=str, help='name of feature to be decoded. Defaults are Xtsd and Ytsd. Must be a tsd variable from behavResources.mat', default='pos')
        p.add_argument('-B', action='store_true', help="rebuilds dataset, even if present.")
    args = parser.parse_args()

    print()
    import shutil
    rows, columns = shutil.get_terminal_size()
    print("|| neuroEncoder by MOBS ||".center(rows))
    print("may 2020".center(rows))
    print("email: t.balenbois@gmail.com".center(rows))
    print()

    # Check if split is not too optimistic
    if args.mode == 'decode':
        print('Decoding of full dataset \n')
    else:
        if args.split > 0.4:
            ii = input('Are you sure you want to leave 40% of data as test set? Seems too much. (y/n)')
            if ii == 'n':
                sys.exit('Stop \n')
            elif ii == 'y':
                print('Ok to big test set \n')
            else:
                raise ValueError('The answer is y or n. Try again \n')

    subprocess.run(["./getTsdFeature.sh", os.path.expanduser(args.path.strip('\'')), "\""+args.target+"\"", "\""+str(args.split)+"\"", "\""+args.whichsplit+"\""])
    print()

    main(args)

    print()
    print()
    print('Encoding over.')
